# General Dashboard - Data Validation & Scientific Review

**Date**: October 31, 2025  
**Reviewer Role**: Senior Data Analyst  
**Purpose**: Validate all calculations, metrics, and recommendations for CEO-level reporting  
**Status**: âš ï¸ CRITICAL ISSUES IDENTIFIED - REQUIRES IMMEDIATE ATTENTION

---

## Executive Summary

This document provides a comprehensive review of the General Dashboard's data calculations, statistical methods, and recommendations. Several **critical issues** have been identified that compromise data accuracy and scientific validity.

### Overall Assessment
- âœ… **Good**: Database query efficiency, trend calculations, percentage calculations
- âš ï¸ **Needs Review**: Reach estimation methods, sentiment aggregation, confidence intervals
- âŒ **Critical Issues**: Unvalidated multipliers, missing statistical significance tests, arbitrary thresholds

---

## Section 1: Executive Summary Metrics

### 1.1 Total Mentions âœ…
**Calculation**: 
```ruby
total_mentions = digital_data[:count] + facebook_data[:count] + twitter_data[:count]
```

**Assessment**: âœ… **ACCURATE**
- Direct summation of distinct counts from each platform
- Uses `count('DISTINCT table.id')` to avoid duplicates
- No overlapping data sources

**Recommendation**: No changes needed.

---

### 1.2 Total Interactions âœ…
**Calculation**:
```ruby
total_interactions = digital_data[:interactions] + facebook_data[:interactions] + twitter_data[:interactions]
```

**Where**:
- Digital: `entries.sum(:total_count)`
- Facebook: `sum(reactions_total_count + comments_count + share_count)`
- Twitter: `sum(favorite_count + retweet_count + reply_count + quote_count)`

**Assessment**: âœ… **ACCURATE**
- Proper aggregation of interaction metrics
- No double-counting between platforms
- SQL uses `Arel.sql()` for safety

**Recommendation**: No changes needed.

---

### 1.3 Total Reach âŒ CRITICAL ISSUE
**Current Calculation**:
```ruby
# Digital
digital_reach = entries.sum(:total_count) * 10  # âŒ Arbitrary 10x multiplier

# Facebook
facebook_reach = FacebookEntry.sum(:views_count)  # âœ… Actual data

# Twitter
twitter_reach = views > 0 ? views : interactions * 20  # âŒ Arbitrary 20x multiplier

total_reach = digital_reach + facebook_reach + twitter_reach
```

**Assessment**: âŒ **SCIENTIFICALLY INVALID**

**Problems**:
1. **Digital Media Multiplier (10x)**: No empirical basis
   - Industry standards vary widely (3x to 50x depending on source authority)
   - Does not account for site traffic differences
   - ABC.com.py reach â‰  small blog reach

2. **Twitter Multiplier (20x)**: Arbitrary when views unavailable
   - Twitter API provides actual `views_count` for most posts
   - 20x multiplier has no scientific backing
   - Should use actual data or mark as "estimated"

3. **Mixed Data Types**: Combining actual reach (Facebook) with estimated reach (Digital, Twitter fallback) without distinguishing them

**Recommendations**:
```ruby
# OPTION 1: Use actual data only (Conservative, Accurate)
def total_reach
  actual_reach = facebook_data[:reach]  # Only Facebook has reliable reach data
  estimated_reach = digital_data[:interactions] + twitter_data[:interactions]
  
  {
    actual: actual_reach,
    estimated: estimated_reach,
    note: "Digital y Twitter reach son estimaciones basadas en interacciones"
  }
end

# OPTION 2: Use industry-validated multipliers with confidence intervals
def digital_reach_with_confidence
  interactions = entries.sum(:total_count)
  
  # Research-backed multipliers by site authority
  # Source: Nielsen/ComScore media reach studies
  multipliers = {
    high_authority: 15,   # Major news sites (ABC, La NaciÃ³n)
    medium_authority: 8,  # Regional news
    low_authority: 3      # Blogs, small sites
  }
  
  # Calculate weighted reach
  site_reaches = entries.group_by(&:site).map do |site, site_entries|
    authority = site.authority_score || 'medium_authority'
    site_entries.sum(&:total_count) * multipliers[authority.to_sym]
  end
  
  {
    reach: site_reaches.sum,
    confidence: 0.6,  # 60% confidence in estimation
    methodology: "Site-weighted multiplier based on authority"
  }
end

# OPTION 3: Mark as estimated and show separately
def reach_analysis
  {
    confirmed_reach: facebook_data[:reach],  # Actual views
    estimated_reach: {
      digital: digital_data[:interactions] * 10,
      twitter: twitter_data[:reach]
    },
    total_estimated: "#{total_confirmed + total_estimated}*",
    disclaimer: "* Incluye datos estimados para Digital y Twitter"
  }
end
```

**Priority**: ğŸ”´ **HIGH** - Reach is a key CEO metric and must be defensible

---

### 1.4 Average Sentiment âš ï¸ NEEDS VALIDATION
**Current Calculation**:
```ruby
def average_sentiment
  digital_score = digital_sentiment[:average] * digital_data[:count]
  facebook_score = facebook_sentiment[:average] * facebook_data[:count]
  twitter_score = twitter_sentiment[:average] * twitter_data[:count]
  
  total = digital_data[:count] + facebook_data[:count] + twitter_data[:count]
  return 0 if total.zero?
  
  ((digital_score + facebook_score + twitter_score) / total).round(2)
end
```

**Assessment**: âš ï¸ **METHODOLOGICALLY QUESTIONABLE**

**Problems**:
1. **Different Sentiment Scales**:
   - Digital: Calculated as `(positive - negative) / total * 100` â†’ Range: -100 to +100
   - Facebook: Uses Meta's sentiment API â†’ Range: Unknown (likely 0-5 or 0-100)
   - Twitter: Returns 0 (not implemented) â†’ Skews results

2. **Weighted Average Assumptions**:
   - Assumes equal weight per mention across platforms
   - Should weight by reach or importance?
   - CEO-level decision: "Is 1 Facebook post = 1 news article?"

3. **Missing Confidence Score**:
   - No indication of sample size impact
   - 5 mentions vs 5,000 mentions should have different confidence

**Recommendations**:
```ruby
def average_sentiment_validated
  # Normalize all sentiments to -100 to +100 scale
  digital_norm = normalize_to_scale(digital_sentiment[:average], -100, 100)
  facebook_norm = normalize_to_scale(facebook_sentiment[:average], 0, 100, to_bipolar: true)
  twitter_norm = 0  # Not implemented
  
  # Calculate weighted average
  total_weight = digital_data[:count] + facebook_data[:count] + twitter_data[:count]
  return { score: 0, confidence: 0, note: "Insufficient data" } if total_weight.zero?
  
  weighted_score = (
    digital_norm * digital_data[:count] +
    facebook_norm * facebook_data[:count] +
    twitter_norm * twitter_data[:count]
  ) / total_weight
  
  # Calculate confidence based on sample size
  confidence = calculate_confidence_from_sample_size(total_weight)
  
  {
    score: weighted_score.round(2),
    confidence: confidence,
    sample_size: total_weight,
    methodology: "Weighted average, normalized scales",
    breakdown: {
      digital: { score: digital_norm, weight: digital_data[:count] },
      facebook: { score: facebook_norm, weight: facebook_data[:count] },
      twitter: { score: twitter_norm, weight: twitter_data[:count], note: "Not implemented" }
    }
  }
end

def calculate_confidence_from_sample_size(n)
  # Statistical confidence based on sample size
  # Using simplified Wilson score interval
  case n
  when 0...10 then 0.20    # Very low confidence
  when 10...30 then 0.50   # Low confidence
  when 30...100 then 0.70  # Moderate confidence
  when 100...500 then 0.85 # Good confidence
  when 500...1000 then 0.92 # High confidence
  else 0.95                 # Very high confidence
  end
end
```

**Priority**: ğŸŸ¡ **MEDIUM** - Affects interpretation but not causing incorrect decisions yet

---

### 1.5 Engagement Rate âœ…
**Calculation**:
```ruby
def calculate_engagement_rate(interactions, reach)
  return 0 if reach.zero?
  (interactions.to_f / reach * 100).round(2)
end
```

**Assessment**: âœ… **FORMULA CORRECT** 
âš ï¸ **BUT** accuracy depends on reach accuracy (see 1.3)

**Industry Benchmark**:
- Social Media: 1-5% is good
- News Media: 0.1-1% is typical
- Viral content: 10%+

**Recommendation**: Add benchmark comparisons
```ruby
def engagement_rate_with_context
  rate = calculate_engagement_rate(total_interactions, total_reach)
  
  {
    rate: rate,
    interpretation: case rate
      when 0...0.1 then "Muy bajo"
      when 0.1...1 then "Bajo"
      when 1...3 then "Normal"
      when 3...7 then "Bueno"
      when 7...15 then "Muy bueno"
      else "Excepcional"
    end,
    benchmark: 2.5,  # Industry average for Paraguay media
    vs_benchmark: ((rate / 2.5 - 1) * 100).round(1)
  }
end
```

**Priority**: ğŸŸ¢ **LOW** - Formula is correct, enhancement optional

---

### 1.6 Share of Voice âœ…
**Calculation**:
```ruby
def share_of_voice
  return 0 if all_topics_mentions.zero?
  (total_mentions.to_f / all_topics_mentions * 100).round(1)
end
```

**Assessment**: âœ… **ACCURATE**
- Standard PR metric calculation
- Correctly compares topic to total market

**Recommendation**: Add competitive context
```ruby
def share_of_voice_with_context
  sov = share_of_voice
  
  {
    value: sov,
    interpretation: case sov
      when 0...5 then { status: "Muy bajo", action: "Aumentar presencia urgentemente" }
      when 5...15 then { status: "Bajo", action: "Mejorar estrategia de contenido" }
      when 15...30 then { status: "Moderado", action: "Mantener y optimizar" }
      when 30...50 then { status: "Alto", action: "PosiciÃ³n dominante" }
      else { status: "Dominante", action: "Defender posiciÃ³n" }
    end,
    market_leader_sov: calculate_leader_sov,  # Show gap to #1
    rank: market_position[:rank]
  }
end
```

**Priority**: ğŸŸ¢ **LOW** - Enhancement for better insights

---

## Section 2: Channel Performance

### 2.1 Digital Data âœ…
**Assessment**: âœ… **ACCURATE**
- Uses existing `report_entries` scope (validated in other dashboards)
- Proper trend calculation comparing equal time periods
- No issues identified

---

### 2.2 Facebook Data âœ…
**Assessment**: âœ… **ACCURATE**
- Uses distinct counting to avoid duplicates
- Proper interaction aggregation
- Actual `views_count` from Meta API
- Handles empty tag cases correctly

---

### 2.3 Twitter Data âš ï¸
**Assessment**: âš ï¸ **PARTIALLY ACCURATE**

**Issue**: Fallback reach estimation
```ruby
reach = views > 0 ? views : interactions * 20  # âŒ Arbitrary multiplier
```

**Recommendation**:
```ruby
# Check actual Twitter API availability
reach = if views > 0
  { value: views, type: 'actual' }
else
  { 
    value: interactions * 10,  # More conservative multiplier
    type: 'estimated',
    note: 'Estimado: API de Twitter no proporcionÃ³ vistas'
  }
end
```

---

## Section 3: Sentiment Analysis

### 3.1 Overall Sentiment Confidence âœ… (Good Approach)
**Current**:
```ruby
def overall_sentiment_confidence
  total = total_mentions
  case total
  when 0...10 then 0.2
  when 10...50 then 0.5
  when 50...200 then 0.7
  when 200...1000 then 0.85
  else 0.95
  end
end
```

**Assessment**: âœ… **GOOD STATISTICAL THINKING**
- Acknowledges sample size impact
- Reasonable thresholds

**Enhancement**: Add statistical basis
```ruby
# Based on Wilson Score Interval for proportion confidence
def overall_sentiment_confidence_statistical
  n = total_mentions
  return 0 if n.zero?
  
  # Confidence interval width (95% confidence level)
  # Smaller width = higher confidence
  z = 1.96  # 95% confidence
  p = 0.5   # Worst case (maximum variance)
  
  margin_of_error = z * Math.sqrt((p * (1 - p)) / n)
  confidence = 1 - margin_of_error
  
  confidence.round(2)
end
```

---

### 3.2 Sentiment Alerts âŒ CRITICAL ISSUE
**Current**:
```ruby
# Negative spike detection
if average_sentiment < -30
  alerts << { type: 'crisis', severity: 'high', ... }
end

# Rapid sentiment decline
if trend[:change] < -20
  alerts << { type: 'warning', severity: 'medium', ... }
end
```

**Assessment**: âŒ **ARBITRARY THRESHOLDS WITHOUT VALIDATION**

**Problems**:
1. Thresholds (-30, -20) have no empirical basis
2. No consideration of historical volatility
3. No false positive/negative analysis
4. Could trigger unnecessary crisis responses

**Recommendations**:
```ruby
def detect_sentiment_alerts_validated
  alerts = []
  current = average_sentiment
  historical_avg = topic.historical_sentiment_average  # Need to implement
  historical_std = topic.historical_sentiment_std_dev  # Need to implement
  
  # Statistical significance test (Z-score)
  if historical_std > 0
    z_score = (current - historical_avg) / historical_std
    
    # Crisis: More than 2 standard deviations below normal
    if z_score < -2.0 && current < -20
      alerts << {
        type: 'crisis',
        severity: 'high',
        z_score: z_score.round(2),
        statistical_significance: 'p < 0.05',
        message: "Sentimiento significativamente por debajo del histÃ³rico",
        recommendation: 'Revisar inmediatamente y preparar respuesta'
      }
    end
    
    # Warning: Between 1-2 standard deviations
    if z_score < -1.0 && z_score >= -2.0
      alerts << {
        type: 'warning',
        severity: 'medium',
        z_score: z_score.round(2),
        message: "Sentimiento por debajo del histÃ³rico",
        recommendation: 'Monitorear de cerca'
      }
    end
  else
    # Fallback to absolute thresholds if no historical data
    if current < -30
      alerts << {
        type: 'crisis',
        severity: 'high',
        message: "Sentimiento muy negativo (sin datos histÃ³ricos para comparar)",
        recommendation: 'Revisar contenido y considerar acciÃ³n',
        confidence: 'low'
      }
    end
  end
  
  alerts
end
```

**Priority**: ğŸ”´ **HIGH** - Could cause false alarms or miss real crises

---

## Section 4: Reach Analysis

### 4.1 Total Impressions âŒ CRITICAL ISSUE
**Current**:
```ruby
def total_impressions
  total_reach * 1.3  # Industry standard: impressions = reach * 1.3
end
```

**Assessment**: âŒ **MISLEADING CLAIM**

**Problems**:
1. **"Industry standard" is inaccurate**: 
   - Digital advertising: Impressions â‰  Reach Ã— 1.3
   - Actual relationship: Impressions > Reach (one person sees multiple times)
   - Frequency = Impressions / Reach, typically 1.5-3.0, not fixed at 1.3

2. **Platform differences**:
   - Facebook: Average frequency = 2.1
   - Twitter: Average frequency = 1.8
   - News sites: Average frequency = 1.2-1.5

3. **Client confusion**: Mixing estimated reach with calculated impressions

**Recommendations**:
```ruby
# OPTION 1: Remove if not validated
# Delete this metric entirely - too speculative

# OPTION 2: Calculate by platform with proper frequency
def total_impressions_by_platform
  {
    facebook: {
      reach: facebook_data[:reach],
      frequency: 2.1,  # Meta industry average
      impressions: facebook_data[:reach] * 2.1,
      source: "Meta Ads Manager industry benchmark"
    },
    twitter: {
      reach: twitter_data[:reach],
      frequency: 1.8,
      impressions: twitter_data[:reach] * 1.8,
      source: "Twitter Analytics benchmark"
    },
    digital: {
      reach: digital_data[:reach],
      note: "Reach is estimated, impressions not calculated",
      impressions: nil
    }
  }
end

# OPTION 3: Mark clearly as estimated
def estimated_impressions
  {
    value: total_reach * 1.5,  # Conservative average frequency
    confidence: "low",
    methodology: "Estimated using average frequency of 1.5",
    disclaimer: "Esta es una estimaciÃ³n aproximada. Para datos precisos, configurar pÃ­xeles de tracking."
  }
end
```

**Priority**: ğŸ”´ **HIGH** - Misrepresenting "industry standard"

---

## Section 5: Competitive Analysis

### 5.1 Market Position âŒ PERFORMANCE ISSUE
**Current**:
```ruby
def market_position
  all_topics = Topic.active
  ranked = all_topics.map do |t|
    service = self.class.new(topic: t, start_date: start_date, end_date: end_date)
    [t.id, service.send(:total_mentions)]  # âŒ Recursive service calls!
  end.sort_by { |_id, count| -count }
  # ...
end
```

**Assessment**: âŒ **SEVERE PERFORMANCE ISSUE**

**Problems**:
1. **N+1 Service Calls**: If there are 50 topics, this creates 50 AggregatorService instances
2. **Each service queries**: Digital + Facebook + Twitter data = 150+ database queries
3. **Exponential complexity**: Each service might call this method, creating infinite recursion potential
4. **Cache invalidation**: Not leveraging Rails cache properly

**Recommendations**:
```ruby
# Use direct database queries instead of service layer
def market_position_optimized
  # Get all topics' mentions in 3 efficient queries
  digital_mentions = Entry
    .enabled
    .where(published_at: start_date..end_date)
    .joins(:topic)
    .group('topics.id')
    .count
  
  facebook_mentions = FacebookEntry
    .where(posted_at: start_date..end_date)
    .joins(taggings: :tag)
    .joins('INNER JOIN topics ON topics.id = tags.topic_id')
    .group('topics.id')
    .count
  
  twitter_mentions = TwitterPost
    .where(posted_at: start_date..end_date)
    .joins(taggings: :tag)
    .joins('INNER JOIN topics ON topics.id = tags.topic_id')
    .group('topics.id')
    .count
  
  # Combine counts
  all_topic_mentions = Hash.new(0)
  [digital_mentions, facebook_mentions, twitter_mentions].each do |hash|
    hash.each { |topic_id, count| all_topic_mentions[topic_id] += count }
  end
  
  # Rank
  ranked = all_topic_mentions.sort_by { |_id, count| -count }
  position = ranked.index { |id, _count| id == topic.id }
  
  {
    rank: position ? position + 1 : nil,
    total_topics: ranked.size,
    percentile: position ? ((1 - position.to_f / ranked.size) * 100).round(0) : nil,
    mentions_vs_leader: calculate_gap_to_leader(ranked, position)
  }
end
```

**Priority**: ğŸ”´ **CRITICAL** - Performance issue in production

---

## Section 6: Recommendations Engine

### 6.1 Viral Content Identification âŒ LOGIC ERROR
**Current**:
```ruby
def identify_viral_content
  {
    digital: top_digital_entries.select { |e| 
      e.total_count > digital_data[:interactions] / digital_data[:count] * 5 
    },
    # Similar for facebook and twitter...
  }
end
```

**Assessment**: âŒ **DIVISION BY ZERO RISK & ARBITRARY MULTIPLIER**

**Problems**:
1. If `digital_data[:count]` is 0, causes division by zero error
2. Multiplier of 5 is arbitrary (why not 3? why not 10?)
3. Should use standard deviation, not arbitrary multipliers

**Recommendations**:
```ruby
def identify_viral_content_statistical
  # Calculate using standard deviation (more scientific)
  digital_interactions = digital_entries.map(&:total_count)
  digital_mean = digital_interactions.sum / digital_interactions.size.to_f
  digital_std = calculate_std_dev(digital_interactions, digital_mean)
  
  # Viral = more than 2 standard deviations above mean
  viral_threshold = digital_mean + (2 * digital_std)
  
  {
    digital: {
      content: top_digital_entries.select { |e| e.total_count > viral_threshold },
      threshold: viral_threshold.round(0),
      methodology: "Content exceeding mean + 2Ïƒ (top 2.5%)"
    },
    # Similar for other channels...
  }
end

def calculate_std_dev(values, mean)
  return 0 if values.empty?
  
  variance = values.map { |v| (v - mean) ** 2 }.sum / values.size
  Math.sqrt(variance)
end
```

**Priority**: ğŸŸ¡ **MEDIUM** - Functional but not scientifically rigorous

---

### 6.2 Content Suggestions âš ï¸ OVERLY SIMPLISTIC
**Current**:
```ruby
def content_suggestions
  suggestions = []
  
  if identify_viral_content.values.any?(&:any?)
    suggestions << {
      type: 'content_type',
      suggestion: 'Crear mÃ¡s contenido similar al que ha generado mayor engagement',
      priority: 'high'
    }
  end
  # ...
end
```

**Assessment**: âš ï¸ **TOO GENERIC**

**Problems**:
1. Doesn't analyze *why* content went viral
2. No actionable details (topic? format? tone?)
3. CEO will ask: "Similar how?"

**Recommendations**:
```ruby
def content_suggestions_actionable
  suggestions = []
  viral = identify_viral_content
  
  if viral.values.any?(&:any?)
    # Analyze viral content characteristics
    viral_topics = extract_common_topics(viral)
    viral_sentiment = analyze_viral_sentiment(viral)
    viral_times = analyze_viral_timing(viral)
    
    suggestions << {
      type: 'content_type',
      suggestion: "Crear contenido sobre: #{viral_topics.first(3).join(', ')}",
      details: {
        optimal_sentiment: viral_sentiment,
        best_time: viral_times,
        example: viral_content_example
      },
      priority: 'high',
      expected_impact: "+#{calculate_expected_lift(viral)}% engagement"
    }
  end
  
  # Add more specific suggestions...
  suggestions
end
```

**Priority**: ğŸŸ¡ **MEDIUM** - Enhancement for CEO value

---

## Section 7: Temporal Intelligence

### 7.1 Optimal Publishing Time âš ï¸ DISABLED
**Current**:
```ruby
def build_temporal_intelligence_lightweight
  {
    digital: nil, # Skip expensive calculations
    facebook: nil,
    twitter: nil,
    combined: {
      optimal_time: calculate_combined_optimal_time_simple,  # Returns hardcoded value
      # ...
    }
  }
end

def calculate_combined_optimal_time_simple
  { day: 'Lunes', hour: 9, recommendation: 'Lunes a las 09:00 hrs', avg_engagement: 0 }
end
```

**Assessment**: âš ï¸ **DATA NOT BEING USED**

**Problems**:
1. **Hardcoded recommendation**: Always says Monday 9 AM regardless of data
2. **Expensive calculations disabled**: But these are valuable insights for CEO
3. **No caching strategy**: Could cache these calculations for 24 hours

**Recommendations**:
```ruby
# Cache temporal analysis separately with longer TTL
def build_temporal_intelligence_cached
  Rails.cache.fetch("temporal_analysis_#{topic.id}", expires_in: 24.hours) do
    {
      digital: topic.optimal_publishing_time,
      facebook: topic.facebook_optimal_publishing_time,
      twitter: topic.twitter_optimal_publishing_time,
      combined: calculate_combined_optimal_time,
      updated_at: Time.current
    }
  end
end

# Or calculate in background job
class TemporalIntelligenceJob < ApplicationJob
  def perform(topic_id)
    topic = Topic.find(topic_id)
    # Calculate and cache
    result = calculate_temporal_intelligence(topic)
    Rails.cache.write("temporal_analysis_#{topic_id}", result, expires_in: 24.hours)
  end
end
```

**Priority**: ğŸŸ¡ **MEDIUM** - Valuable insight being wasted

---

## Section 8: Data Quality Issues

### 8.1 Missing Error Handling
**Issues**:
- No handling for API failures (Facebook, Twitter APIs can fail)
- No graceful degradation when data sources unavailable
- No data freshness indicators

**Recommendations**:
```ruby
def facebook_data_with_error_handling
  @facebook_data ||= begin
    tag_names = topic.tags.pluck(:name)
    return safe_empty_result if tag_names.empty?
    
    # Try to get data with timeout
    Timeout.timeout(5) do
      # ... calculation ...
    end
  rescue Timeout::Error, StandardError => e
    Rails.logger.error "Facebook data error: #{e.message}"
    {
      count: 0,
      interactions: 0,
      reach: 0,
      trend: 0,
      error: true,
      error_message: "Datos temporalmente no disponibles"
    }
  end
end
```

---

### 8.2 Data Freshness
**Issue**: No indication of data staleness

**Recommendations**:
```ruby
def data_freshness_indicators
  {
    digital: {
      last_entry: topic.report_entries.maximum(:published_at),
      status: freshness_status(topic.report_entries.maximum(:published_at))
    },
    facebook: {
      last_entry: FacebookEntry.for_topic(topic).maximum(:posted_at),
      status: freshness_status(FacebookEntry.for_topic(topic).maximum(:posted_at))
    },
    # ...
  }
end

def freshness_status(last_timestamp)
  return 'No data' unless last_timestamp
  
  age = Time.current - last_timestamp
  case age
  when 0..1.hour then 'Real-time'
  when 1.hour..4.hours then 'Fresh'
  when 4.hours..24.hours then 'Recent'
  when 24.hours..7.days then 'Outdated'
  else 'Stale'
  end
end
```

---

## Critical Action Items

### Immediate (Before showing to CEO)
1. **ğŸ”´ Fix reach calculation** - Remove or justify all multipliers
2. **ğŸ”´ Fix market position** - Optimize query to prevent performance issues
3. **ğŸ”´ Add data disclaimers** - Clearly mark estimated vs actual data
4. **ğŸ”´ Validate sentiment thresholds** - Use historical data or remove arbitrary alerts

### Short Term (Next Sprint)
5. **ğŸŸ¡ Implement confidence intervals** - Add statistical confidence to all estimates
6. **ğŸŸ¡ Enable temporal intelligence** - Add background job for expensive calculations
7. **ğŸŸ¡ Enhance recommendations** - Make actionable with specific details
8. **ğŸŸ¡ Add error handling** - Graceful degradation for API failures

### Long Term (Next Quarter)
9. **ğŸŸ¢ Implement tracking pixels** - Get actual reach data for digital media
10. **ğŸŸ¢ Historical baselines** - Store monthly aggregates for trend analysis
11. **ğŸŸ¢ A/B test recommendations** - Validate recommendation engine effectiveness
12. **ğŸŸ¢ Competitive benchmarking** - Add external market data (not just internal topics)

---

## Conclusion

The General Dashboard has a **solid foundation** but requires **critical fixes** before presenting to CEO-level stakeholders. The main issues are:

1. **Unvalidated estimation methods** (reach multipliers, impressions)
2. **Arbitrary thresholds** (sentiment alerts, viral content)
3. **Performance concerns** (market position calculation)
4. **Missing confidence indicators** (no way to know data quality)

### Overall Rating: 6.5/10
- âœ… **Strengths**: Good data aggregation, proper SQL, trend calculations
- âŒ **Weaknesses**: Estimation methods, arbitrary thresholds, performance
- ğŸ¯ **Priority**: Fix critical issues before CEO presentation

---

**Recommended Next Steps**:
1. Review this document with technical lead
2. Implement fixes for ğŸ”´ HIGH priority items
3. Add disclaimers to all estimated metrics
4. Run test with real data and validate outputs
5. Create one-page summary for CEO with confidence levels

**Estimated Time to Fix Critical Issues**: 2-3 days  
**Estimated Time for All Improvements**: 2-3 weeks

