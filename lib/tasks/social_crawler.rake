# frozen_string_literal: true

require 'nokogiri'
require 'open-uri'
require 'parallel'

# List of site IDs to ignore completely
IGNORED_SITE_IDS = [142].freeze

desc 'Crawl URLs from unlinked social media posts (Twitter and Facebook) and link them to entries'
task social_crawler: :environment do
  puts '=' * 80
  puts 'SOCIAL MEDIA CRAWLER'
  puts '=' * 80
  puts "Started at: #{Time.current}"
  puts ''

  # Statistics
  stats = {
    twitter_processed: 0,
    twitter_linked: 0,
    twitter_created: 0,
    twitter_skipped: 0,
    facebook_processed: 0,
    facebook_linked: 0,
    facebook_created: 0,
    facebook_skipped: 0,
    errors: 0
  }

  #------------------------------------------------------------------------------
  # Process Twitter Posts
  #------------------------------------------------------------------------------
  puts 'ðŸ“± Processing Twitter Posts...'
  puts '-' * 80

  # Find Twitter posts of type "Link" without an associated entry
  twitter_posts = TwitterPost.where(entry_id: nil)
                             .where("payload IS NOT NULL")
                             .where("LENGTH(payload) > 10")
                             .order(created_at: :desc)

  puts "Found #{twitter_posts.count} unlinked Twitter posts to process"
  puts ''

  Parallel.each(twitter_posts, in_threads: 5) do |post|
    ActiveRecord::Base.connection_pool.with_connection do
      begin
        # Extract URL from post
        url = extract_twitter_url(post)

        if url.blank?
          stats[:twitter_skipped] += 1
          next
        end

        # Normalize URL
        url = normalize_url(url)

        puts ''
        puts "[TWITTER] Processing: #{url}"

        # Find associated site
        site = find_site_for_url(url)

        unless site
          puts "  â†’ No site found for URL"
          stats[:twitter_skipped] += 1
          next
        end

        puts "  â†’ Site: #{site.name}"

        # Check if site is in ignored list
        if IGNORED_SITE_IDS.include?(site.id)
          puts "  â†’ Skipping: Site is in ignored list (ID: #{site.id})"
          stats[:twitter_skipped] += 1
          next
        end

        # Apply URL filters
        unless url_matches_site_filters?(url, site)
          stats[:twitter_skipped] += 1
          next
        end

        # Check if entry already exists
        existing_entry = Entry.find_by(url: url)

        if existing_entry
          puts "  âœ“ Entry already exists (ID: #{existing_entry.id})"
          post.update!(entry: existing_entry)
          puts "  âœ“ Linked twitter post to existing entry"
          stats[:twitter_linked] += 1
          stats[:twitter_processed] += 1
          next
        end

        # Fetch and process the URL
        puts "  â†’ Fetching content..."
        doc = fetch_page(url, site)

        unless doc
          puts "  âœ— Failed to fetch content"
          stats[:errors] += 1
          next
        end

        # Create entry
        entry = Entry.create!(url: url, site: site)
        puts "  âœ“ Created entry (ID: #{entry.id})"

        # Extract basic info
        result = WebExtractorServices::ExtractBasicInfo.call(doc)
        if result.success?
          entry.update!(result.data)
          puts "  âœ“ Basic info extracted: #{entry.title&.truncate(60)}"
        else
          puts "  âœ— ERROR BASIC: #{result.error}"
        end

        # Extract content
        if site.content_filter.present?
          result = WebExtractorServices::ExtractContent.call(doc, site.content_filter)
          if result.success?
            entry.update!(result.data)
            puts "  âœ“ Content extracted (#{result.data[:content]&.length} chars)"
          else
            puts "  âœ— ERROR CONTENT: #{result.error}"
          end
        end

        # Extract date
        result = WebExtractorServices::ExtractDate.call(doc)
        if result.success?
          entry.update!(result.data)
          puts "  âœ“ Date extracted: #{result.published_at}"
        else
          puts "  âœ— ERROR DATE: #{result.error}"
        end

        # Extract tags
        result = WebExtractorServices::ExtractTags.call(entry.id)
        if result.success?
          entry.tag_list.add(result.data)
          entry.save!
          tags = result.data.is_a?(Array) ? result.data : [result.data]
          puts "  âœ“ Tags extracted: #{tags.join(', ')}"
        else
          puts "  âœ— ERROR TAGGER: #{result.error}"
        end

        # Extract title tags
        result = WebExtractorServices::ExtractTitleTags.call(entry.id)
        if result.success?
          entry.tag_list.add(result.data)
          entry.save!
          tags = result.data.is_a?(Array) ? result.data : [result.data]
          puts "  âœ“ Title tags extracted: #{tags.join(', ')}"
        else
          puts "  âœ— ERROR TITLE TAGGER: #{result.error}"
        end

        # Update Facebook stats
        result = FacebookServices::UpdateStats.call(entry.id)
        if result.success?
          entry.update!(result.data)
          puts "  âœ“ Stats updated"
        end

        # Link post to entry
        post.update!(entry: entry)
        puts "  âœ“ Linked twitter post to entry"

        stats[:twitter_created] += 1
        stats[:twitter_processed] += 1
        puts "  âœ… Successfully processed!"

      rescue StandardError => e
        puts "  âœ— ERROR: #{e.message}"
        puts "  #{e.backtrace.first(3).join("\n  ")}"
        stats[:errors] += 1
      end
    end
  end

  puts ''
  puts '=' * 80

  #------------------------------------------------------------------------------
  # Process Facebook Posts
  #------------------------------------------------------------------------------
  puts 'ðŸ“˜ Processing Facebook Posts...'
  puts '-' * 80

  # Find Facebook posts of type "Link" without an associated entry
  facebook_posts = FacebookEntry.where(entry_id: nil)
                                .where(attachment_type: 'share')
                                .where("attachment_url IS NOT NULL")
                                .order(created_at: :desc)

  puts "Found #{facebook_posts.count} unlinked Facebook posts to process"
  puts ''

  Parallel.each(facebook_posts, in_threads: 5) do |post|
    ActiveRecord::Base.connection_pool.with_connection do
      begin
        url = post.attachment_url

        if url.blank?
          stats[:facebook_skipped] += 1
          next
        end

        # Normalize URL
        url = normalize_url(url)

        puts ''
        puts "[FACEBOOK] Processing: #{url}"

        # Find associated site
        site = find_site_for_url(url)

        unless site
          puts "  â†’ No site found for URL"
          stats[:facebook_skipped] += 1
          next
        end

        puts "  â†’ Site: #{site.name}"

        # Check if site is in ignored list
        if IGNORED_SITE_IDS.include?(site.id)
          puts "  â†’ Skipping: Site is in ignored list (ID: #{site.id})"
          stats[:facebook_skipped] += 1
          next
        end

        # Apply URL filters
        unless url_matches_site_filters?(url, site)
          stats[:facebook_skipped] += 1
          next
        end

        # Check if entry already exists
        existing_entry = Entry.find_by(url: url)

        if existing_entry
          puts "  âœ“ Entry already exists (ID: #{existing_entry.id})"
          post.update!(entry: existing_entry)
          puts "  âœ“ Linked facebook post to existing entry"
          stats[:facebook_linked] += 1
          stats[:facebook_processed] += 1
          next
        end

        # Fetch and process the URL
        puts "  â†’ Fetching content..."
        doc = fetch_page(url, site)

        unless doc
          puts "  âœ— Failed to fetch content"
          stats[:errors] += 1
          next
        end

        # Create entry
        entry = Entry.create!(url: url, site: site)
        puts "  âœ“ Created entry (ID: #{entry.id})"

        # Extract basic info
        result = WebExtractorServices::ExtractBasicInfo.call(doc)
        if result.success?
          entry.update!(result.data)
          puts "  âœ“ Basic info extracted: #{entry.title&.truncate(60)}"
        else
          puts "  âœ— ERROR BASIC: #{result.error}"
        end

        # Extract content
        if site.content_filter.present?
          result = WebExtractorServices::ExtractContent.call(doc, site.content_filter)
          if result.success?
            entry.update!(result.data)
            puts "  âœ“ Content extracted (#{result.data[:content]&.length} chars)"
          else
            puts "  âœ— ERROR CONTENT: #{result.error}"
          end
        end

        # Extract date
        result = WebExtractorServices::ExtractDate.call(doc)
        if result.success?
          entry.update!(result.data)
          puts "  âœ“ Date extracted: #{result.published_at}"
        else
          puts "  âœ— ERROR DATE: #{result.error}"
        end

        # Extract tags
        result = WebExtractorServices::ExtractTags.call(entry.id)
        if result.success?
          entry.tag_list.add(result.data)
          entry.save!
          tags = result.data.is_a?(Array) ? result.data : [result.data]
          puts "  âœ“ Tags extracted: #{tags.join(', ')}"
        else
          puts "  âœ— ERROR TAGGER: #{result.error}"
        end

        # Extract title tags
        result = WebExtractorServices::ExtractTitleTags.call(entry.id)
        if result.success?
          entry.tag_list.add(result.data)
          entry.save!
          tags = result.data.is_a?(Array) ? result.data : [result.data]
          puts "  âœ“ Title tags extracted: #{tags.join(', ')}"
        else
          puts "  âœ— ERROR TITLE TAGGER: #{result.error}"
        end

        # Update Facebook stats
        result = FacebookServices::UpdateStats.call(entry.id)
        if result.success?
          entry.update!(result.data)
          puts "  âœ“ Stats updated"
        end

        # Link post to entry
        post.update!(entry: entry)
        puts "  âœ“ Linked facebook post to entry"

        stats[:facebook_created] += 1
        stats[:facebook_processed] += 1
        puts "  âœ… Successfully processed!"

      rescue StandardError => e
        puts "  âœ— ERROR: #{e.message}"
        puts "  #{e.backtrace.first(3).join("\n  ")}"
        stats[:errors] += 1
      end
    end
  end

  #------------------------------------------------------------------------------
  # Final Statistics
  #------------------------------------------------------------------------------
  puts ''
  puts '=' * 80
  puts 'SUMMARY'
  puts '=' * 80
  puts "Twitter Posts:"
  puts "  - Processed: #{stats[:twitter_processed]}"
  puts "  - Linked to existing: #{stats[:twitter_linked]}"
  puts "  - New entries created: #{stats[:twitter_created]}"
  puts "  - Skipped: #{stats[:twitter_skipped]}"
  puts ''
  puts "Facebook Posts:"
  puts "  - Processed: #{stats[:facebook_processed]}"
  puts "  - Linked to existing: #{stats[:facebook_linked]}"
  puts "  - New entries created: #{stats[:facebook_created]}"
  puts "  - Skipped: #{stats[:facebook_skipped]}"
  puts ''
  puts "Total Errors: #{stats[:errors]}"
  puts ''
  puts "Finished at: #{Time.current}"
  puts '=' * 80
end

#------------------------------------------------------------------------------
# Helper Methods
#------------------------------------------------------------------------------

def extract_twitter_url(post)
  return nil unless post.payload.present?

  payload = JSON.parse(post.payload)

  # Look for URLs in entities
  urls = payload.dig('legacy', 'entities', 'urls') || []

  # Return the first expanded URL
  urls.first&.dig('expanded_url')
rescue JSON::ParserError, StandardError
  nil
end

def normalize_url(url)
  return nil if url.blank?

  # Parse and normalize the URL
  uri = URI.parse(url)

  # Remove common tracking parameters
  if uri.query
    params = URI.decode_www_form(uri.query)
    # Remove fbclid, utm_*, gclid, etc.
    params.reject! { |k, _v| k =~ /^(fbclid|utm_|gclid|_ga)/ }
    uri.query = params.empty? ? nil : URI.encode_www_form(params)
  end

  # Rebuild URL
  uri.to_s
rescue URI::InvalidURIError, StandardError
  url
end

def find_site_for_url(url)
  return nil if url.blank?

  uri = URI.parse(url)
  domain = uri.host

  # Try exact match first
  site = Site.find_by('url LIKE ?', "%#{domain}%")

  # Try base domain if no match
  unless site
    base_domain = domain.split('.').last(2).join('.')
    site = Site.find_by('url LIKE ?', "%#{base_domain}%")
  end

  site
rescue URI::InvalidURIError, StandardError
  nil
end

def url_matches_site_filters?(url, site)
  return false unless url.present? && site.present?

  begin
    # Check for unwanted file extensions
    unwanted_extensions = /.*\.(jpeg|jpg|gif|png|pdf|mp3|mp4|mpeg|zip|rar|exe|dmg)$/i
    if url.match?(unwanted_extensions)
      puts "  â†’ Skipping: Unwanted file extension"
      return false
    end

    # Check for unwanted directories
    # Only match complete directory segments (surrounded by / or at start/end)
    unwanted_directories = %w[
      blackhole wp-login wp-admin galerias fotoblog radios page
      etiqueta categoria category pagina auth wp-content img tag
      contacto programa date feed author
    ]
    # Match directories as complete path segments: /directory/ or /directory or directory/
    directory_pattern = /(?:\/|^)(?:#{unwanted_directories.join('|')})(?:\/|$)/i
    if url.match?(directory_pattern)
      puts "  â†’ Skipping: Contains unwanted directory"
      return false
    end

    # Check positive filter (if exists)
    if site.filter.present?
      filter_regex = Regexp.new(site.filter)
      unless url.match?(filter_regex)
        puts "  â†’ Skipping: Does not match site's positive filter (#{site.filter})"
        return false
      end
    end

    # Check negative filter (if exists)
    if site.negative_filter.present?
      negative_regex = Regexp.new(site.negative_filter)
      if url.match?(negative_regex)
        puts "  â†’ Skipping: Matches site's negative filter (#{site.negative_filter})"
        return false
      end
    end

    true
  rescue StandardError => e
    puts "  âš  Error validating URL filters: #{e.message}"
    false
  end
end

def fetch_page(url, site = nil)
  return nil if url.blank?

  # If site requires JavaScript, use proxy directly
  if site&.is_js?
    puts "  â†’ Site requires JS, using scrape.do proxy..."
    return fetch_via_proxy(url, site)
  end

  # Try normal fetch first
  begin
    response = URI.parse(url).open(
      'User-Agent' => 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    )

    return Nokogiri::HTML(response.read.force_encoding('UTF-8'))
  rescue OpenURI::HTTPError => e
    # If we get 403 Forbidden, try proxy (site might be protected)
    http_code = e.io.status[0].to_i rescue nil
    if http_code == 403 || e.message.include?('403')
      puts "  âš  Got 403 Forbidden, retrying with scrape.do proxy..."
      return fetch_via_proxy(url, site)
    end

    puts "  âš  Fetch error: #{e.message}"
    nil
  rescue StandardError => e
    puts "  âš  Fetch error: #{e.message}"
    nil
  end
end

def fetch_via_proxy(url, site = nil)
  begin
    proxy_client = ProxyCrawlerServices::ProxyClient.new

    # Use site's content_filter as waitSelector if available
    wait_selector = site&.content_filter.present? ? site.content_filter : nil

    if wait_selector.present?
      puts "  â†’ Using waitSelector: #{wait_selector}"
    end

    response = proxy_client.fetch(url, wait_selector: wait_selector)

    unless response.success?
      puts "  âš  Proxy fetch error: #{response.error}"
      return nil
    end

    puts "  âœ“ Successfully fetched via proxy (#{response.body.size} bytes)"
    Nokogiri::HTML(response.body.force_encoding('UTF-8'))
  rescue StandardError => e
    puts "  âš  Proxy error: #{e.message}"
    nil
  end
end

